{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoImageProcessor, AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import textwrap\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some utils.\n",
    "\n",
    "def split_reasoning(text, tags):\n",
    "    new_parts = {None: text}\n",
    "\n",
    "    for tag in tags:\n",
    "        parts = new_parts\n",
    "        new_parts = dict()\n",
    "\n",
    "        for k, v in parts.items():\n",
    "            if tag in v:\n",
    "                s = v.split(tag)\n",
    "                new_parts[k] = s[0]\n",
    "                new_parts[tag] = s[1]\n",
    "                # print(tag, s)\n",
    "            else:\n",
    "                new_parts[k] = v\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "class CotTag(enum.Enum):\n",
    "    TASK = \"TASK:\"\n",
    "    PLAN = \"PLAN:\"\n",
    "    VISIBLE_OBJECTS = \"VISIBLE OBJECTS:\"\n",
    "    SUBTASK_REASONING = \"SUBTASK REASONING:\"\n",
    "    SUBTASK = \"SUBTASK:\"\n",
    "    MOVE_REASONING = \"MOVE REASONING:\"\n",
    "    MOVE = \"MOVE:\"\n",
    "    GRIPPER_POSITION = \"GRIPPER POSITION:\"\n",
    "    ACTION = \"ACTION:\"\n",
    "\n",
    "\n",
    "def get_cot_tags_list():\n",
    "    return [\n",
    "        CotTag.TASK.value,\n",
    "        CotTag.PLAN.value,\n",
    "        CotTag.VISIBLE_OBJECTS.value,\n",
    "        CotTag.SUBTASK_REASONING.value,\n",
    "        CotTag.SUBTASK.value,\n",
    "        CotTag.MOVE_REASONING.value,\n",
    "        CotTag.MOVE.value,\n",
    "        CotTag.GRIPPER_POSITION.value,\n",
    "        CotTag.ACTION.value,\n",
    "    ]\n",
    "\n",
    "def name_to_random_color(name):\n",
    "    return [(hash(name) // (256**i)) % 256 for i in range(3)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_gripper(img, pos_list, img_size=(640, 480)):\n",
    "    for i, pos in enumerate(reversed(pos_list)):\n",
    "        pos = resize_pos(pos, img_size)\n",
    "        scale = 255 - int(255 * i / len(pos_list))\n",
    "        cv2.circle(img, pos, 6, (0, 0, 0), -1)\n",
    "        cv2.circle(img, pos, 5, (scale, scale, 255), -1)\n",
    "\n",
    "def get_metadata(reasoning):\n",
    "    metadata = {\"gripper\": [[0, 0]], \"bboxes\": dict()}\n",
    "\n",
    "    if f\" {CotTag.GRIPPER_POSITION.value}\" in reasoning:\n",
    "        gripper_pos = reasoning[f\" {CotTag.GRIPPER_POSITION.value}\"]\n",
    "        gripper_pos = gripper_pos.split(\"[\")[-1]\n",
    "        gripper_pos = gripper_pos.split(\"]\")[0]\n",
    "        gripper_pos = [int(x) for x in gripper_pos.split(\",\")]\n",
    "        gripper_pos = [(gripper_pos[2 * i], gripper_pos[2 * i + 1]) for i in range(len(gripper_pos) // 2)]\n",
    "        metadata[\"gripper\"] = gripper_pos\n",
    "\n",
    "    if f\" {CotTag.VISIBLE_OBJECTS.value}\" in reasoning:\n",
    "        for sample in reasoning[f\" {CotTag.VISIBLE_OBJECTS.value}\"].split(\"]\"):\n",
    "            obj = sample.split(\"[\")[0]\n",
    "            if obj == \"\":\n",
    "                continue\n",
    "            coords = [int(n) for n in sample.split(\"[\")[-1].split(\",\")]\n",
    "            metadata[\"bboxes\"][obj] = coords\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def resize_pos(pos, img_size):\n",
    "    return [(x * size) // 256 for x, size in zip(pos, img_size)]\n",
    "\n",
    "def draw_bboxes(img, bboxes, img_size=(640, 480)):\n",
    "    for name, bbox in bboxes.items():\n",
    "        show_name = name\n",
    "        # show_name = f'{name}; {str(bbox)}'\n",
    "\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            resize_pos((bbox[0], bbox[1]), img_size),\n",
    "            resize_pos((bbox[2], bbox[3]), img_size),\n",
    "            name_to_random_color(name),\n",
    "            1,\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img,\n",
    "            show_name,\n",
    "            resize_pos((bbox[0], bbox[1] + 6), img_size),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            (255, 255, 255),\n",
    "            1,\n",
    "            cv2.LINE_AA,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reinaldoyang/miniconda3/envs/ecot/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "# Load Processor & VLA\n",
    "path_to_converted_ckpt = \"Embodied-CoT/ecot-openvla-7b-bridge\"\n",
    "processor = AutoProcessor.from_pretrained(path_to_converted_ckpt, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    path_to_converted_ckpt,\n",
    "    # attn_implementation=\"flash_attention_2\",  # [Optional] Requires `flash_attn`\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant.\n",
      "The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "USER: What action should the robot take to place the watermelon on the towel? ASSISTANT: TASK:\n",
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    ")\n",
    "\n",
    "def get_openvla_prompt(instruction: str) -> str:\n",
    "    return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: TASK:\"\n",
    "\n",
    "INSTRUCTION = \"place the watermelon on the towel\"\n",
    "prompt = get_openvla_prompt(INSTRUCTION)\n",
    "image = Image.open(\"./test_obs.png\")\n",
    "print(prompt.replace(\". \", \".\\n\"))\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got numpy.ndarray)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# === BFLOAT16 MODE ===\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# inputs[\"input_ids\"] = inputs[\"input_ids\"][:, 1:]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# --- Process input ---\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass PIL Image\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# --- Move tensors to GPU (FP32) ---\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Embodied-CoT/ecot-openvla-7b-bridge/492b3dbf3df380f6da333f86ce06dab028176166/processing_prismatic.py:207\u001b[0m, in \u001b[0;36mPrismaticProcessor.__call__\u001b[0;34m(self, text, images, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    189\u001b[0m     text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     return_tensors: Optional[Union[\u001b[38;5;28mstr\u001b[39m, TensorType]] \u001b[38;5;241m=\u001b[39m TensorType\u001b[38;5;241m.\u001b[39mPYTORCH,\n\u001b[1;32m    195\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    Preprocess a given (batch) of text/images for a Prismatic VLM; forwards text to the underlying LLM's tokenizer,\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    forwards images to PrismaticImageProcessor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    @return: BatchFeature with keys for `input_ids`, `attention_mask` and `pixel_values`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    208\u001b[0m     text_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    209\u001b[0m         text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, padding\u001b[38;5;241m=\u001b[39mpadding, truncation\u001b[38;5;241m=\u001b[39mtruncation, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# [Validate] Need same number of images and text inputs!\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Embodied-CoT/ecot-openvla-7b-bridge/492b3dbf3df380f6da333f86ce06dab028176166/processing_prismatic.py:170\u001b[0m, in \u001b[0;36mPrismaticImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: Union[Image\u001b[38;5;241m.\u001b[39mImage, List[Image\u001b[38;5;241m.\u001b[39mImage]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Embodied-CoT/ecot-openvla-7b-bridge/492b3dbf3df380f6da333f86ce06dab028176166/processing_prismatic.py:164\u001b[0m, in \u001b[0;36mPrismaticImageProcessor.preprocess\u001b[0;34m(self, images, return_tensors, **_)\u001b[0m\n\u001b[1;32m    161\u001b[0m     images \u001b[38;5;241m=\u001b[39m [images]\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Apply `self.img_transform` to each image (will return list of torch.Tensors); stack into \"batched\" Tensor\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_transform(img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images])\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Return BatchFeature =>> note that for compatibility, constructor expects Dict[str, np.ndarray], so we convert\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: pixel_values\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()}, tensor_type\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Embodied-CoT/ecot-openvla-7b-bridge/492b3dbf3df380f6da333f86ce06dab028176166/processing_prismatic.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    161\u001b[0m     images \u001b[38;5;241m=\u001b[39m [images]\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Apply `self.img_transform` to each image (will return list of torch.Tensors); stack into \"batched\" Tensor\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images])\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Return BatchFeature =>> note that for compatibility, constructor expects Dict[str, np.ndarray], so we convert\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: pixel_values\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()}, tensor_type\u001b[38;5;241m=\u001b[39mreturn_tensors)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Embodied-CoT/ecot-openvla-7b-bridge/492b3dbf3df380f6da333f86ce06dab028176166/processing_prismatic.py:138\u001b[0m, in \u001b[0;36mPrismaticImageProcessor.apply_transform\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    136\u001b[0m img_idx \u001b[38;5;241m=\u001b[39m TVF\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtvf_resize_params[idx])\n\u001b[1;32m    137\u001b[0m img_idx \u001b[38;5;241m=\u001b[39m TVF\u001b[38;5;241m.\u001b[39mcenter_crop(img_idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtvf_crop_params[idx])\n\u001b[0;32m--> 138\u001b[0m img_idx_t \u001b[38;5;241m=\u001b[39m \u001b[43mTVF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m img_idx_t \u001b[38;5;241m=\u001b[39m TVF\u001b[38;5;241m.\u001b[39mnormalize(img_idx_t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtvf_normalize_params[idx])\n\u001b[1;32m    140\u001b[0m imgs_t\u001b[38;5;241m.\u001b[39mappend(img_idx_t)\n",
      "File \u001b[0;32m~/miniconda3/envs/ecot/lib/python3.10/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got numpy.ndarray)"
     ]
    }
   ],
   "source": [
    "# === BFLOAT16 MODE ===\n",
    "inputs = processor(prompt, image).to(device, dtype=torch.bfloat16)\n",
    "# inputs[\"input_ids\"] = inputs[\"input_ids\"][:, 1:]\n",
    "\n",
    "# Run OpenVLA Inference\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "action, generated_ids = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False, max_new_tokens=1024)\n",
    "generated_text = processor.batch_decode(generated_ids)[0]\n",
    "print(f\"Time: {time.time() - start_time:.4f} || Action: {action}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [f\" {tag}\" for tag in get_cot_tags_list()]\n",
    "reasoning = split_reasoning(generated_text, tags)\n",
    "text = [tag + reasoning[tag] for tag in [' TASK:',' PLAN:',' SUBTASK REASONING:',' SUBTASK:',\n",
    "                                        ' MOVE REASONING:',' MOVE:', ' VISIBLE OBJECTS:', ' GRIPPER POSITION:'] if tag in reasoning]\n",
    "metadata = get_metadata(reasoning)\n",
    "bboxes = {}\n",
    "for k, v in metadata[\"bboxes\"].items():\n",
    "    if k[0] == \",\":\n",
    "        k = k[1:]\n",
    "    bboxes[k.lstrip().rstrip()] = v\n",
    "\n",
    "caption = \"\"\n",
    "for t in text:\n",
    "    wrapper = textwrap.TextWrapper(width=80, replace_whitespace=False) \n",
    "    word_list = wrapper.wrap(text=t) \n",
    "    caption_new = ''\n",
    "    for ii in word_list[:-1]:\n",
    "        caption_new = caption_new + ii + '\\n      '\n",
    "    caption_new += word_list[-1]\n",
    "\n",
    "    caption += caption_new.lstrip() + \"\\n\\n\"\n",
    "\n",
    "base = Image.fromarray(np.ones((480, 640, 3), dtype=np.uint8) * 255)\n",
    "draw = ImageDraw.Draw(base)\n",
    "font = ImageFont.load_default(size=14) # big text\n",
    "color = (0,0,0) # RGB\n",
    "draw.text((30, 30), caption, color, font=font)\n",
    "\n",
    "img_arr = np.array(image)\n",
    "draw_gripper(img_arr, metadata[\"gripper\"])\n",
    "draw_bboxes(img_arr, bboxes)\n",
    "\n",
    "text_arr = np.array(base)\n",
    "\n",
    "reasoning_img = Image.fromarray(np.concatenate([img_arr, text_arr], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
